---
title: "Tutorial: Forecasting Using R"
author: "Manuela da Cruz Chadreque"
date: "20 de Maio de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Forecasting Using R

## Exploring and visualizing time series in R
Creating time series objects in R


```{r}
setwd("C:/Users/maiam/Dropbox/ABRIL-MAIO 2020/PYTHON/PORTFOLIO/TUTORIALS/r/Forecasting Using R")
library(readxl)
# Read the data from Excel into R
mydata <- read_excel("exercise1.xlsx")

# Look at the first few lines of mydata
head(mydata)

# Create a ts object called myts
myts <- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)
```
Time series plots
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 

```{r}
library(forecast)
# Plot the data with facetting
autoplot(myts, facets = TRUE)

# Plot the data without facetting
autoplot(myts, facets = FALSE)

# Plot the three series
autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)

# Find the outlier in the gold series
goldoutlier <- which.max(gold)

# Look at the seasonal frequencies of the three series
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```
Seasonal plots

```{r}
# Load the fpp2 package
library(fpp2)

# Create plots of the a10 data
autoplot(a10)
ggseasonplot(a10)

# Produce a polar coordinate season plot for the a10 data
ggseasonplot(a10, polar = TRUE)

# Restrict the ausbeer data to start in 1992
beer <- window(ausbeer, start=1992)

# Make plots of the beer data
autoplot(beer)
ggsubseriesplot(beer)
```

## Trends, seasonality and cyclicity

Autocorrelation of non-seasonal time series

```{r}
# Create an autoplot of the oil data
autoplot(oil)

# Create a lag plot of the oil data
gglagplot(oil)

# Create an ACF plot of the oil data
ggAcf(oil)
```

Autocorrelation of seasonal and cyclic time series

```{r}
# Plot the annual sunspot numbers
autoplot(sunspot.year)
ggAcf(sunspot.year)

# Save the lag corresponding to maximum autocorrelation
maxlag_sunspot <- 1

# Plot the traffic on the Hyndsight blog
autoplot(hyndsight)
ggAcf(hyndsight)


# Save the lag corresponding to maximum autocorrelation
maxlag_hyndsight <- 7
```

White noise

Stock prices and white noise
, white noise is a term that describes purely random data. You can conduct a Ljung-Box test using the function Box.test

There is a well-known result in economics called the "Efficient Market Hypothesis" that states that asset prices reflect all available information. A consequence of this is that the daily changes in stock prices should behave like white noise (ignoring dividends, interest rates and transaction costs). The consequence for forecasters is that the best forecast of the future price is the current price.

```{r}
# Plot the original series
autoplot(goog)

# Plot the differenced series
autoplot(diff(goog))

# ACF of the differenced series
ggAcf(diff(goog))
# Ljung-Box test of the differenced series
Box.test(diff(goog), lag = 10, type = "Ljung")
```

#Forecasts and potential futures


Naive forecasting methods
 a forecast is the mean or median of simulated futures of a time series.
 
```{r}
# Use naive() to forecast the goog series
fcgoog <- naive(goog, h=20)

# Plot and summarize the forecasts
autoplot(fcgoog)
summary(fcgoog)

# Use snaive() to forecast the ausbeer series
fcbeer <- snaive(ausbeer,h=16)

# Plot and summarize the forecasts
autoplot(fcbeer)
summary(fcbeer)
```
Fitted values and residuals
Checking time series residuals
When applying a forecasting method, it is important to always check that the residuals are well-behaved (i.e., no outliers or patterns) and resemble white noise. The prediction intervals are computed assuming that the residuals are also normally distributed. You can use the checkresiduals() function to verify these characteristics; it will give the results of a Ljung-Box test.

```{r}
# Check the residuals from the naive forecasts applied to the goog series
goog %>% naive() %>% checkresiduals()

# Do they look like white noise (TRUE or FALSE)
googwn <-TRUE

# Check the residuals from the seasonal naive forecasts applied to the ausbeer series

ausbeer %>% snaive() %>% checkresiduals()

# Do they look like white noise (TRUE or FALSE)
beerwn <- FALSE
```

##Evaluating forecast accuracy of non-seasonal methods

a training set is a data set that is used to discover possible relationships. A test set is a data set that is used to verify the strength of these potential relationships. When you separate a data set into these parts, you generally allocate more of the data for training, and less for testing.


````{r}
# Create the training data as train
train <- subset(gold, end = 1000)

# Compute naive forecasts and save to naive_fc
naive_fc <- naive(train, h = 108)

# Compute mean forecasts and save to mean_fc
mean_fc <- meanf(train, h = 108)

# Use accuracy() to compute RMSE statistics
accuracy(naive_fc, gold)
accuracy(mean_fc, gold)

# Assign one of the two forecasts as bestforecasts
bestforecasts <-naive_fc
```

#Evaluating forecast accuracy of seasonal methods

window() function specifies the start and end of a time series using the relevant times rather than the index values. Either of those two arguments can be formatted as a vector like c(year, period) which you have also previously used as an argument for ts(). Again, period refers to quarter here.

```{r}
library(forecast)
library(fpp2)
library(fpp)

# Create three training series omitting the last 1, 2, and 3 years
train1 <- window(vn[, "Melbourne"], end = c(2010,4))
train2 <- window(vn[, "Melbourne"], end = c(2009,4))
train3 <- window(vn[, "Melbourne"], end = c(2008,4))

# Produce forecasts using snaive()
fc1 <- snaive(train1, h = 4)
fc2 <- snaive(train2, h = 4)
fc3 <- snaive(train3, h = 4)

# Use accuracy() to compare the MAPE of each series
accuracy(fc1, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc2, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc3, vn[, "Melbourne"])["Test set", "MAPE"]
```

## Time series cross-validation
Using tsCV() for time series cross-validation
The tsCV() function computes time series cross-validation errors. It requires you to specify the time series, the forecast method, and the forecast horizon
```{r}

library(fpp2)
# Compute cross-validated errors for up to 8 steps ahead
e <- tsCV(goog, forecastfunction = naive, h = 8)

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

#Exponential smoothing
The ses() function produces forecasts obtained using simple exponential smoothing (SES). The parameters are estimated using least squares estimation.
```{r}
# Use ses() to forecast the next 10 years of winning times
fc <- ses(marathon, h = 10)

# Use summary() to see the model parameters
summary(fc)

# Use autoplot() to plot the forecasts
autoplot(fc)

# Add the one-step forecasts for the training data to the plot
autoplot(fc) + autolayer(fitted(fc))
```
SES vs naive

Let's review the process:

<ol>
<li>First, import and load your data. Determine how much of your data you want to allocate to training, and how much to testing; the sets should not overlap.
<li>Subset the data to create a training set, which you will use as an argument in your forecasting function(s). Optionally, you can also create a test set to use later.
<li>Compute forecasts of the training set using whichever forecasting function(s) you choose, and set h equal to the number of values you want to forecast, which is also the length of the test set.
<li>To view the results, use the accuracy() function with the forecast as the first argument and original data (or test set) as the second.
<li>Pick a measure in the output, such as RMSE or MAE, to evaluate the forecast(s); a smaller error indicates higher accuracy.
</ol>
```{r}
# Create a training set using subset()
train <- subset(marathon, end = length(marathon) - 20)

# Compute SES and naive forecasts, save to fcses and fcnaive
fcses <- ses(train, h = 20)
fcnaive <- naive(train, h = 20)

# Calculate forecast accuracy measures
accuracy(fcses, marathon)
accuracy(fcnaive, marathon)

# Save the best forecasts as fcbest
fcbest <- fcnaive
```
Exponential smoothing methods with trend
Holt's trend methods
```{r}
# Produce 10 year forecasts of austa using holt()
fcholt <- holt(austa,h=10)

# Look at fitted model using summary()
summary(fcholt)

# Plot the forecasts
autoplot(fcholt)

# Check that the residuals look like white noise
checkresiduals(fcholt)
```
Exponential smoothing methods with trend and seasonality

Holt-Winters with monthly data
```{r}
# Plot the data
autoplot(a10)

# Produce 3 year forecasts
fc <- hw(a10, seasonal ="multiplicative" , h = 36)

# Check if residuals look like white noise
checkresiduals(fc)
whitenoise <- FALSE

# Plot forecasts
autoplot(fc)
```

Holt-Winters method with daily data

```{r}
# Create training data with subset()
train <- subset(hyndsight, end =337)

# Holt-Winters additive forecasts as fchw
fchw <- hw(train, seasonal = "additive", h = 28)

# Seasonal naive forecasts as fcsn
fcsn <- snaive(train,h=28)

# Find better forecasts with accuracy()
accuracy(fchw,hyndsight)
accuracy(fcsn,hyndsight)

# Plot the better forecasts
autoplot(fchw)
```

Automatic forecasting with exponential smoothing

```{r}
# Fit ETS model to austa in fitaus
fitaus <- ets(austa)

# Check residuals
checkresiduals(fitaus)

# Plot forecasts
autoplot(forecast(fitaus))

# Repeat for hyndsight data in fiths
fiths <- ets(hyndsight)
checkresiduals(fiths)
autoplot(forecast(fiths))

# Which model(s) fails test? (TRUE or FALSE)
fitausfail <- FALSE
fithsfail <- TRUE
```

# ETS vs seasonal naive

compare ETS forecasts against seasonal naive forecasting for 20 years of cement
```{r}
# Function to return ETS forecasts
fets <- function(y, h) {
  forecast(ets(y), h = h)
}

# Apply tsCV() for both methods
e1 <- tsCV(cement, fets, h =4)
e2 <- tsCV(cement, snaive, h =4)

# Compute MSE of resulting errors (watch out for missing values)
mean(e1^2, na.rm=TRUE)
mean(e2^2, na.rm=TRUE)

# Copy the best forecast MSE
bestmse <- 0.02910046
```

When does ETS fail?

```{r}
# Plot the lynx series
autoplot(lynx)

# Use ets() to model the lynx series
fit <- ets(lynx)

# Use summary() to look at model and parameters
summary(fit)
# Plot 20-year forecasts of the lynx series
fit %>% forecast(h=20) %>% autoplot()
```